import tempfile
import subprocess
import os
import zipfile

import gc
import dask.dataframe as dd
import pandas as pd

from custom_code.upload_file_to_gcs import upload_file_to_gcs
from custom_code.download_file_from_gcs import download_file_from_gcs
from custom_code.load_data import load_data
from custom_code.load_wa_forecast import load_wa_forecast
from custom_code.load_product import load_product
from custom_code.process_results import process_results
from custom_code.settings import RUNTAG, PROJECT, BUCKET, DATA_DIR, CODE_DIR, RESULTS_DIR, PARAMS
from custom_code.metrics import mean_huber, mape, wmape
from datascience.google_cloud.data_proc_executor import DataProcExecutor
from datascience.monitoring.logger import get_logger

LOGGER = get_logger()
LOGGER.setLevel('DEBUG')

DATA = False
WA_FORECAST = False
PRODUCT = False
# Train and predict in a single step (avoid intermediate I/O files)
TRAIN_AND_PREDICT = False
RESULTS = True


def zipdir(path, ziph):
    # ziph is zipfile handle
    for root, dirs, files in os.walk(path):
        for file in files:
            ziph.write(os.path.join(root, file))


def lightgbm_upload_files():
    """
    Uploads the code files to GCP - for the "calculate" command

    config: Config object as generated by Config()
    """
    LOGGER.info('Uploading code files to GS bucket for calculation')
    upload_file_to_gcs(PROJECT, BUCKET, './initialize_cluster.sh', '{}/initialize_cluster.sh'.format(CODE_DIR))
    upload_file_to_gcs(PROJECT, BUCKET,
                       './lightgbm_framework.py', '{}/lightgbm_framework.py'.format(CODE_DIR))
    zipf = zipfile.ZipFile('./custom_code.zip', 'w', zipfile.ZIP_DEFLATED)
    zipdir('./custom_code/', zipf)
    zipf.close()
    upload_file_to_gcs(PROJECT, BUCKET, './custom_code.zip', '{}/custom_code.zip'.format(CODE_DIR))
    LOGGER.info('Code files uploaded')


def lightgbm_calculate(config, cluster_name, disk_config, cores, memory):
    """
    Computes lightgbm train and prediction steps on a cluster in GCP.

    config: The configuration object, generated by Config()
    cluster_name: The name that should be used for the cluster on google cloud
    disk_config: Configuration for diskConfig
    cores: Number of CPU cores
    memory: Memory for the machine provided in GB's
    """

    executor = DataProcExecutor(config.project, cluster_name)
    bucket_path = 'gs://coolblue-ds-demand-forecast-dev/marnix/lightgbm/code/'
    script_path = bucket_path + 'lightgbm_framework.py'
    init_path = bucket_path + 'initialize_cluster.sh'
    python_files = ['custom_code.zip']
    python_files = [bucket_path + f for f in python_files]

    init_actions = [init_path]

    job_args = str({
        'project': config.project
    })

    try:
        executor.create_single_node_dataproc_cluster(disk_config=disk_config, init_actions=init_actions,
                                                     cores=cores, memory=memory)
        executor.execute_dataproc_job(script=script_path, args=job_args, python_files=python_files)
    finally:
        executor.delete_dataproc_cluster()

if __name__ == "__main__":

    ############################## Steps running locally ##############################
    ###################################################################################
    LOGGER.info('Starting experiment: {}'.format(RUNTAG))

    if DATA:
        LOGGER.info('Loading data from BQ')
        data_df = load_data()
        LOGGER.info('Writing data to GCS')
        with open(tempfile.NamedTemporaryFile().name, 'w') as tf:
            data_df.to_hdf('{}.h5'.format(tf.name), 'data_df', index=False)
            upload_file_to_gcs(PROJECT, BUCKET, '{}.h5'.format(tf.name), '{}/actual.h5'.format(DATA_DIR))
        subprocess.call(['rm', '-f', tf.name])
        subprocess.call(['rm', '-f', '{}.h5'.format(tf.name)])
        del data_df
        gc.collect()

    if WA_FORECAST:
        LOGGER.info('Computing WA forecast from BQ')
        wa_forecast_df = load_wa_forecast()
        LOGGER.info('Writing WA to GCS')
        with open(tempfile.NamedTemporaryFile().name, 'w') as tf:
            wa_forecast_df.to_hdf('{}.h5'.format(tf.name), 'wa_forecast_df', index=False)
            upload_file_to_gcs(PROJECT, BUCKET, '{}.h5'.format(tf.name), '{}/wa.h5'.format(DATA_DIR))
        subprocess.call(['rm', '-f', tf.name])
        subprocess.call(['rm', '-f', '{}.h5'.format(tf.name)])
        del wa_forecast_df
        gc.collect()

    if PRODUCT:
        LOGGER.info('Loading Product from BQ')
        product_df = load_product()
        LOGGER.info('Writing Product to GCS')
        with open(tempfile.NamedTemporaryFile().name, 'w') as tf:
            product_df.to_hdf('{}.h5'.format(tf.name), 'product_df', index=False)
            upload_file_to_gcs(PROJECT, BUCKET, '{}.h5'.format(tf.name), '{}/product.h5'.format(DATA_DIR))
        subprocess.call(['rm', '-f', tf.name])
        subprocess.call(['rm', '-f', '{}.h5'.format(tf.name)])
        del product_df
        gc.collect()

    ############################ Steps running in the cloud ###########################
    ###################################################################################

    if TRAIN_AND_PREDICT:

        class Config:
            project = 'coolblue-bi-platform-dev'

        kwargs = {
            'cluster_name': 'lightgbm64',
            'disk_config': {'bootDiskSizeGb': 1024, 'numLocalSsds': 0}, # 'bootDiskType': 'pd-standard'
            'cores': 64, # 64
            'memory': 412 # 412
        }

        lightgbm_upload_files()
        lightgbm_calculate(Config(), **kwargs)

    ############################## Steps running locally ##############################
    ###################################################################################

    if RESULTS:

        LOGGER.info('Calculating final results with WA')
        LOGGER.info('Reading results and feature importance per fold')
        results_df = dd.read_csv('gs://{}/{}/results_*_{}.csv'.format(BUCKET, RESULTS_DIR, RUNTAG))
        results_df = results_df.compute()
        # Drop overlap (due to growing folds we have the same observations n times in there)
        # All test folds are preserved as they appear first in the frame
        # Note that at any point in time the train fold (in-sample fit) is correct as it only uses data from the past
        # This does mean that the latest fold would produce a better in-sample fit throughout the sample which is not stored
        results_df.drop_duplicates(subset=['product_id', 'date'], keep='first', inplace=True)
        features_importance_df = dd.read_csv('gs://{}/{}/overall_importance_{}.csv'.format(BUCKET, RESULTS_DIR, RUNTAG))
        features_importance_df = features_importance_df.compute()
        LOGGER.info('Processing results')
        process_results(results_df, features_importance_df, PARAMS)
        LOGGER.info('Writing results with WA to GCS')
        with open(tempfile.NamedTemporaryFile().name, 'w') as tf:
            results_df.to_csv('{}.csv'.format(tf.name), index=False)
            upload_file_to_gcs(PROJECT, BUCKET, '{}.csv'.format(tf.name),
                               '{}/{}/results_with_wa_{}.csv'.format(BUCKET, RESULTS_DIR, RUNTAG))
        subprocess.call(['rm', '-f', tf.name])
        subprocess.call(['rm', '-f', '{}.csv'.format(tf.name)])

        del results_df, features_importance_df
        gc.collect()
        print('Finished')
